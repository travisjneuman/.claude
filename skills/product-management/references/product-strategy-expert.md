---
name: product-strategy-expert
description: Fortune 50-level Product Management expertise covering product strategy, roadmap planning, feature prioritization, customer research, analytics, A/B testing, product-market fit analysis, and lifecycle management. Use when developing product strategies, prioritizing features, planning roadmaps, conducting customer research, analyzing product metrics, designing experiments, or managing product lifecycles.
---

# Product Strategy Expert

This skill provides Fortune 50-caliber Product Management expertise across the complete product lifecycle. Use this skill when working on product strategy, feature prioritization, roadmap planning, customer research, product analytics, experimentation, or lifecycle management.

## When to Use This Skill

Invoke this skill when the user needs help with:

- Defining product vision, strategy, and positioning
- Creating or evaluating product roadmaps
- Prioritizing features using frameworks (RICE, ICE, MoSCoW, Kano, Value vs. Effort)
- Designing and executing customer research (interviews, surveys, usability tests)
- Analyzing product metrics and defining success criteria
- Designing and analyzing A/B tests and experiments
- Assessing product-market fit
- Managing products across lifecycle stages (intro, growth, maturity, decline)
- Creating product requirement documents (PRDs) or user stories
- Stakeholder communication and alignment
- Competitive analysis and market positioning

## Core Product Management Frameworks

### 1. Product Vision & Strategy

**Vision Definition Framework:**

1. **Purpose**: Why does this product exist? What problem does it solve?
2. **Target Audience**: Who are the primary and secondary user personas?
3. **Value Proposition**: What unique value does this product deliver?
4. **Differentiation**: How is this different from alternatives?
5. **Success Vision**: What does success look like in 1, 3, 5 years?

**Strategy Canvas:**

- Map competitive factors on two axes: investment level vs. industry standard
- Identify factors to: Eliminate, Reduce, Raise, Create (ERRC Grid)
- Define unique value curve that differentiates from competitors

**North Star Metric:**

- Identify the single metric that best captures core product value
- Must reflect customer value delivered (not just business metrics)
- Should be leading indicator of long-term success
- Examples: Weekly Active Users engaging with core feature, Time to Value, Customer Lifetime Value

### 2. Feature Prioritization Frameworks

**RICE Scoring (Reach × Impact × Confidence ÷ Effort):**

- **Reach**: How many users/customers will this impact per time period?
  - Measure in users/quarter or customers/month
  - Use actual data when possible, educated estimates when not
- **Impact**: How much will this impact each user? (Scale: 3=Massive, 2=High, 1=Medium, 0.5=Low, 0.25=Minimal)
  - Massive: Core value prop enhancement, removes major blocker
  - High: Significant improvement to key workflow
  - Medium: Moderate improvement or nice-to-have enhancement
  - Low: Small convenience or edge case fix
- **Confidence**: How confident are you in your estimates? (100%=High, 80%=Medium, 50%=Low)
  - High: Strong data, validated customer feedback, proven patterns
  - Medium: Some data, limited validation, reasonable assumptions
  - Low: Mostly assumptions, unvalidated hypothesis, high uncertainty
- **Effort**: How much total team effort in person-months?
  - Include design, engineering, testing, launch effort
  - Account for dependencies and risks
- **RICE Score = (Reach × Impact × Confidence) ÷ Effort**

**ICE Scoring (Impact × Confidence × Ease):**

- Faster alternative to RICE for rapid prioritization
- **Impact**: Value delivered to users/business (Scale 1-10)
- **Confidence**: Certainty in impact estimate (Scale 1-10)
- **Ease**: Inverse of effort required (Scale 1-10, where 10=easiest)
- **ICE Score = (Impact + Confidence + Ease) ÷ 3**

**MoSCoW Method:**

- **Must Have**: Critical for launch, non-negotiable, core value
- **Should Have**: Important but not critical, can defer if needed
- **Could Have**: Nice to have, include if time/resources allow
- **Won't Have**: Explicitly out of scope for this iteration

**Kano Model (Customer Satisfaction Analysis):**

- **Basic Needs**: Expected features; absence causes dissatisfaction
- **Performance Needs**: More is better; linear satisfaction relationship
- **Delighters**: Unexpected features; presence creates satisfaction
- **Indifferent**: Customers don't care either way
- **Reverse**: Some customers want it, others actively dislike it

**Value vs. Effort Matrix (2×2):**

- **Quick Wins** (High Value, Low Effort): Do first
- **Big Bets** (High Value, High Effort): Plan strategically, resource properly
- **Fill-ins** (Low Value, Low Effort): Do when capacity available
- **Time Sinks** (Low Value, High Effort): Avoid or deprioritize

### 3. Roadmap Planning

**Strategic Roadmap Structure:**

- **Time Horizons**: Now (0-3mo), Next (3-6mo), Later (6-12mo), Future (12mo+)
- **Themes**: Group features into strategic themes, not just feature lists
- **Outcomes**: Focus on desired outcomes, not just outputs
- **Dependencies**: Map technical and market dependencies
- **Assumptions**: Document key assumptions that could invalidate plans

**Roadmap Communication Formats:**

- **Executive/Board**: Theme-based, outcome-focused, strategic context
- **Sales/Customer-Facing**: Benefit-oriented, timeline ranges, avoid specifics
- **Engineering/Internal**: Feature-specific, technical dependencies, sprint planning
- **Cross-Functional**: Balanced detail, clear ownership, milestone-based

**Roadmap Review Cadence:**

- **Monthly**: Tactical adjustments, priority shifts, scope refinements
- **Quarterly**: Strategic alignment, outcome assessment, major pivots
- **Annually**: Vision refresh, multi-year planning, market repositioning

### 4. Customer Research Methods

**Discovery Research (What to build):**

**1. Customer Interviews:**

- **Purpose**: Understand deep needs, motivations, pain points
- **Sample Size**: 5-15 per segment for qualitative insights
- **Structure**:
  - Introduction (5min): Build rapport, explain purpose
  - Context (10min): Understand current workflow/situation
  - Deep Dive (30min): Explore problems, needs, behaviors
  - Ideation (10min): Reaction to potential solutions
  - Close (5min): Follow-up permissions, thank you
- **Key Principles**:
  - Ask open-ended questions: "Tell me about..." vs. "Do you like..."
  - Focus on past behavior, not future intentions
  - Probe for specific examples: "Walk me through the last time..."
  - Avoid leading questions or pitching solutions
- **Analysis**: Thematic coding, identify patterns across 3+ interviews

**2. Jobs-to-be-Done (JTBD) Framework:**

- **Job Statement Format**: "When [situation], I want to [motivation], so I can [outcome]"
- **Identify**: Functional jobs (tasks), Emotional jobs (feelings), Social jobs (perception)
- **Forces Analysis**:
  - Push (problems with current solution)
  - Pull (attraction to new solution)
  - Anxiety (fears about new solution)
  - Habits (attachment to current solution)

**3. User Surveys:**

- **Purpose**: Quantify problems, validate hypotheses, segment users
- **Sample Size**: 100+ for statistical significance per segment
- **Question Types**:
  - Screening: Qualify respondents
  - Multiple Choice: Easy analysis, limited insight
  - Likert Scale: Measure intensity (1-5 or 1-7 scales)
  - Open-Ended: Rich insights, harder to analyze
- **Best Practices**:
  - One concept per question
  - Avoid double-barreled questions
  - Randomize answer order to prevent bias
  - Include attention checks for quality control
  - Keep surveys under 10 minutes

**Validation Research (Is this the right solution):**

**1. Usability Testing:**

- **Purpose**: Identify usability issues, validate design decisions
- **Sample Size**: 5-8 users per iteration (diminishing returns after 5)
- **Types**:
  - Moderated: Researcher present, can probe and adapt
  - Unmoderated: Scalable, less expensive, limited probing
  - Remote: Geographic flexibility, natural environment
  - In-Person: Rich observation, body language insights
- **Protocol**:
  - Task-based scenarios (not feature tours)
  - Think-aloud protocol
  - Minimal intervention by facilitator
  - Observe without defending design
- **Metrics**: Task success rate, time on task, error rate, satisfaction (SUS score)

**2. Concept Testing:**

- **Purpose**: Validate product concepts before building
- **Methods**:
  - Landing page tests (measure sign-up intent)
  - Fake door tests (gauge interest in non-existent feature)
  - Prototype testing (interactive mockups)
  - Wizard of Oz (manual backend, real frontend)
- **Evaluation Criteria**:
  - Comprehension: Do they understand it?
  - Relevance: Does it solve their problem?
  - Differentiation: Is it better than alternatives?
  - Intent: Would they use/buy it?

### 5. Product Analytics

**Analytics Strategy Framework:**

**1. Metrics Hierarchy:**

- **North Star Metric**: Single metric capturing core value
- **Primary Metrics**: 3-5 key health indicators
- **Secondary Metrics**: Supporting metrics per feature/workflow
- **Guardrail Metrics**: Metrics that shouldn't degrade (quality, trust, performance)

**2. AARRR Pirate Metrics Framework:**

- **Acquisition**: How do users discover us? (Traffic sources, CAC, conversion rate)
- **Activation**: Do they have a great first experience? (Onboarding completion, time-to-value, aha moment)
- **Retention**: Do they come back? (DAU/MAU, cohort retention, churn rate)
- **Revenue**: How do we monetize? (ARPU, LTV, conversion to paid)
- **Referral**: Do they tell others? (NPS, viral coefficient, referral rate)

**3. Event Tracking Design:**

- **Event Naming Convention**: [Object]\_[Action] (e.g., "report_generated", "filter_applied")
- **Properties to Capture**:
  - User properties: segment, tenure, plan type
  - Event properties: feature, method, outcome
  - Context: platform, version, A/B variant
- **Implementation Checklist**:
  - Document event catalog in centralized location
  - Validate events fire correctly in all scenarios
  - Set up dashboards before launch
  - Define alert thresholds for anomalies

**4. Cohort Analysis:**

- **Retention Cohorts**: Group by sign-up date, analyze retention over time
- **Behavioral Cohorts**: Group by actions taken, compare outcomes
- **Example Questions**:
  - Does Week 1 retention predict Month 6 retention?
  - Do users who complete onboarding retain better?
  - Which acquisition channels have best long-term retention?

**5. Funnel Analysis:**

- **Map Critical Paths**: Sign-up flow, onboarding, core workflow, upgrade path
- **Measure Drop-off**: Conversion rate at each step
- **Identify Leaks**: Where do most users abandon?
- **Optimize**: A/B test improvements at highest-drop-off steps

**6. Product KPIs by Type:**

**Engagement Products (Social, Content):**

- Daily/Weekly/Monthly Active Users (DAU/WAU/MAU)
- Session frequency and duration
- Content creation rate
- Stickiness (DAU/MAU ratio)

**SaaS Products:**

- Monthly Recurring Revenue (MRR) growth
- Churn rate (revenue and logo)
- Net Revenue Retention (NRR)
- Customer Acquisition Cost (CAC) to Lifetime Value (LTV) ratio

**E-commerce:**

- Conversion rate
- Average Order Value (AOV)
- Cart abandonment rate
- Repeat purchase rate

**Marketplace:**

- Gross Merchandise Volume (GMV)
- Take rate
- Liquidity (supply/demand balance)
- Cross-side network effects

### 6. A/B Testing & Experimentation

**Experiment Design Framework:**

**1. Hypothesis Formation:**

- **Template**: "If we [change], then [metric] will [improve] because [reason]"
- **Example**: "If we add social proof to pricing page, then conversion rate will increase by 10% because users will trust us more"
- **Requirements**:
  - Specific change being tested
  - Measurable success metric
  - Quantified expected impact
  - Clear causal reasoning

**2. Experiment Setup:**

- **Randomization**: Users randomly assigned to variants
- **Sample Size Calculation**:
  - Baseline conversion rate
  - Minimum Detectable Effect (MDE): Smallest change worth detecting
  - Statistical power: Typically 80% (probability of detecting real effect)
  - Significance level: Typically 95% (α = 0.05)
  - Use online calculator or formula: n = 16σ²/δ² per variant
- **Duration**:
  - Run for at least one full business cycle (week)
  - Achieve required sample size
  - Minimum 1 week, maximum 4 weeks typically

**3. Variants:**

- **Control**: Current experience (baseline)
- **Treatment**: New experience being tested
- **Multiple Variants**: Test 2-3 variants max to maintain power
- **Avoid**: Too many variants (reduces statistical power)

**4. Metrics:**

- **Primary Metric**: The one metric that determines success/failure
- **Secondary Metrics**: Additional metrics to understand impact
- **Guardrail Metrics**: Metrics that must not degrade significantly
- **Leading Indicators**: Quick signals before primary metric moves

**5. Analysis:**

- **Statistical Significance**: p-value < 0.05 (95% confidence)
- **Practical Significance**: Effect size large enough to matter
- **Segment Analysis**: Did effect vary by user segment?
- **Novelty Effect**: Could results be due to temporary change response?
- **Network Effects**: Could treatment affect control group?

**6. Decision Framework:**

- **Ship**: Stat sig + practical sig + no negative guardrails
- **Iterate**: Directionally positive but not stat sig, or mixed results
- **Kill**: No effect or negative impact
- **Investigate**: Unexpected results, large variance, segment differences

**7. Common Experiment Pitfalls:**

- **Peeking**: Checking results before reaching sample size (increases false positives)
- **Multiple Comparisons**: Testing too many metrics without correction
- **Stopping Early**: Declaring winner before statistical significance
- **Novelty Effect**: Users react to change itself, not sustained value
- **Interaction Effects**: Running multiple overlapping experiments
- **Sample Ratio Mismatch**: Unequal distribution suggests implementation bug

### 7. Product-Market Fit Assessment

**PMF Measurement Framework:**

**1. Sean Ellis PMF Survey:**

- **Question**: "How would you feel if you could no longer use [product]?"
  - Very disappointed (strong PMF signal)
  - Somewhat disappointed (neutral)
  - Not disappointed (no PMF)
- **Threshold**: >40% "very disappointed" indicates PMF
- **Follow-up**: Ask "very disappointed" group what they'd use instead

**2. Retention Cohort Analysis:**

- **Leading Indicator**: Flattening retention curve
- **Metrics**:
  - Month 1 to Month 3 retention >30% (SaaS)
  - Week 4 retention >20% (consumer)
  - Cohorts improving over time
- **Visualization**: Plot cohort retention curves; PMF = flattening curves

**3. Growth Accounting:**

- **Formula**: Net Growth = New Users + Resurrected Users - Churned Users
- **PMF Indicators**:
  - Organic growth outpacing paid acquisition
  - Resurrection rate increasing (users coming back)
  - Churn rate decreasing over time
  - Word-of-mouth driving meaningful acquisition

**4. NPS (Net Promoter Score):**

- **Question**: "How likely are you to recommend [product] to a friend?" (0-10 scale)
- **Calculation**: % Promoters (9-10) - % Detractors (0-6)
- **Benchmarks**: >50 excellent, 30-50 good, 0-30 needs work, <0 major issues
- **Limitations**: Correlation with growth, not causation; use with other signals

**5. Qualitative Signals:**

- Users expressing strong emotional attachment
- Unsolicited testimonials and referrals
- Users hacking together workarounds to keep using product
- Competitors trying to copy features
- Press/analyst coverage without PR push

**PMF Stages:**

**Stage 1: Pre-PMF**

- High churn, flat retention curves
- Users need convincing to try product
- Growth is purely paid/forced
- Tepid customer feedback
- **Focus**: Find core value prop, iterate rapidly on positioning/features

**Stage 2: Early PMF**

- Some users can't live without it
- Retention improving but still weak overall
- Word-of-mouth starting
- Specific segment loves it, others lukewarm
- **Focus**: Double down on core users, understand what's working

**Stage 3: Strong PMF**

- Retention curves flattening
- Organic growth accelerating
- Clear value prop resonating
- Users actively recruiting others
- **Focus**: Scale what's working, expand carefully to adjacent segments

### 8. Product Lifecycle Management

**Lifecycle Stages & Strategies:**

**1. Introduction Stage:**

- **Characteristics**: Low sales, high costs, minimal competition
- **Goals**: Build awareness, establish early adopters, prove concept
- **Metrics**: User acquisition, activation rate, early retention
- **Strategies**:
  - Focus on innovators and early adopters
  - Emphasize education and onboarding
  - Gather intensive user feedback
  - Iterate rapidly on core value prop
  - Build distribution channels
- **Common Mistakes**: Scaling too early, ignoring feedback, premature optimization

**2. Growth Stage:**

- **Characteristics**: Rapid sales growth, increasing competition, improving margins
- **Goals**: Scale user base, establish market position, achieve profitability
- **Metrics**: Growth rate, market share, CAC:LTV ratio, NRR
- **Strategies**:
  - Scale marketing and sales
  - Add features for broader appeal
  - Improve operational efficiency
  - Build defensible moats (network effects, data, brand)
  - Enter adjacent markets or segments
- **Common Mistakes**: Ignoring core users, feature bloat, losing focus

**3. Maturity Stage:**

- **Characteristics**: Slowing growth, intense competition, stable market share
- **Goals**: Maximize profit, defend market share, extend lifecycle
- **Metrics**: Profitability, customer satisfaction, retention, operational efficiency
- **Strategies**:
  - Optimize pricing and packaging
  - Focus on retention and expansion
  - Segment and personalize
  - Improve efficiency and reduce costs
  - Diversify revenue streams
  - Innovate to extend lifecycle
- **Common Mistakes**: Complacency, under-investing in innovation, alienating core users

**4. Decline Stage:**

- **Characteristics**: Falling sales, eroding margins, exits by competitors
- **Goals**: Maximize remaining value, manage graceful exit or pivot
- **Metrics**: Profitability, churn rate, support costs
- **Strategies**:
  - Harvest: Reduce costs, maximize profit from remaining users
  - Divest: Sell product to interested buyer
  - Sunset: Wind down gracefully, migrate users
  - Pivot: Transform into new product
- **Common Mistakes**: Waiting too long, abrupt shutdown, ignoring loyal customers

## Product Requirements Documentation

**PRD Structure (One-Pager Format):**

**1. Executive Summary (3-4 sentences):**

- What: One-line description
- Why: Core problem being solved
- Who: Target users
- Success: How we'll measure it

**2. Background & Context:**

- Current situation and pain points
- Supporting data (customer feedback, metrics, research)
- Strategic alignment (company/product goals)

**3. Goals & Success Metrics:**

- Primary goal and success metric
- Secondary goals and metrics
- Guardrail metrics
- Target values and timeline

**4. User Stories:**

- **Format**: "As a [persona], I want to [action], so that [benefit]"
- **Acceptance Criteria**: Specific conditions that must be met
- **Priority**: Must/Should/Could Have

**5. Solution Overview:**

- High-level description of proposed solution
- Key user flows and interactions
- Core functionality
- Out of scope (explicit non-goals)

**6. Design & Technical Considerations:**

- Mockups or wireframes (links)
- Technical constraints or dependencies
- Integration requirements
- Scalability considerations

**7. Launch Plan:**

- Rollout strategy (phased, full launch, A/B test)
- Success criteria for each phase
- Risk mitigation plans
- Training/documentation needs

**8. Open Questions:**

- Unresolved decisions
- Areas needing more research
- Dependencies on other teams

## Stakeholder Communication

**Communication Matrix:**

**Executive Stakeholders:**

- **Frequency**: Monthly or quarterly
- **Format**: High-level strategy, outcomes, key decisions needed
- **Content**: Business impact, strategic alignment, risks, resource needs
- **Length**: 1-page or 10-minute presentation

**Engineering/Design Teams:**

- **Frequency**: Weekly or bi-weekly
- **Format**: Detailed specs, user stories, acceptance criteria
- **Content**: Requirements, edge cases, technical constraints, priorities
- **Length**: Detailed PRDs, ongoing collaborative refinement

**Sales/Customer Success:**

- **Frequency**: Monthly roadmap reviews, ad-hoc updates
- **Format**: Customer-facing benefits, competitive positioning
- **Content**: Release timeline ranges, value propositions, beta opportunities
- **Length**: Brief updates with Q&A

**Customers/Users:**

- **Frequency**: Major releases, significant changes
- **Format**: Benefits-focused announcements, migration guides
- **Content**: What's changing, why it matters, how to adopt
- **Length**: Release notes, blog posts, in-app notifications

**Communication Best Practices:**

- Adapt content and framing to audience
- Lead with "so what" and benefits
- Use data to support decisions
- Be transparent about tradeoffs
- Under-promise, over-deliver on timelines
- Create feedback loops

## Advanced Product Techniques

**1. Opportunity Solution Trees:**

- **Structure**: Desired outcome → Opportunities → Solutions → Experiments
- **Purpose**: Visually map path from business goals to tactical experiments
- **Process**:
  1. Define desired outcome (single, measurable)
  2. Identify opportunities (customer needs, pain points, desires)
  3. Generate solution ideas for each opportunity
  4. Design experiments to test solutions
- **Benefits**: Ensures solutions tie to outcomes, surfaces assumptions, enables parallel exploration

**2. Continuous Discovery:**

- **Weekly touchpoints** with customers (interviews, usability tests, data analysis)
- **Cross-functional involvement** (PM, designer, engineer)
- **Document insights** in shared repository
- **Integrate into roadmap** planning cycles
- **Goal**: Stay connected to customers, validate continuously

**3. Value Proposition Canvas:**

- **Customer Profile**:
  - Jobs to be done
  - Pains (obstacles, risks, negative emotions)
  - Gains (outcomes, benefits, aspirations)
- **Value Map**:
  - Products & Services
  - Pain relievers (how we address pains)
  - Gain creators (how we create gains)
- **Fit**: Align pain relievers and gain creators with customer pains and gains

**4. Business Model Canvas:**

- **9 Building Blocks**: Customer segments, value propositions, channels, customer relationships, revenue streams, key resources, key activities, key partnerships, cost structure
- **Use**: Design, challenge, or pivot business model
- **Workshop Format**: Collaborative team exercise on large canvas

## Industry-Specific Considerations

**B2B SaaS Products:**

- Longer sales cycles require demo-ready features
- Prioritize enterprise needs (security, compliance, integrations, admin controls)
- Land-and-expand motion influences feature prioritization
- Champion vs. economic buyer vs. end user needs
- Implementation and onboarding are part of product

**Consumer Products:**

- Viral loops and shareability in feature design
- Retention and engagement paramount
- Network effects if applicable
- Platform considerations (iOS, Android, web)
- App store optimization and discovery

**Marketplace Products:**

- Chicken-and-egg problem in early stage
- Balance supply and demand side needs
- Liquidity as key metric
- Geographic expansion strategy
- Trust and safety critical

**Hardware/Physical Products:**

- Long development cycles require high conviction
- Manufacturing and supply chain considerations
- Difficult to iterate post-launch
- Warranty and support costs
- Retail vs. direct-to-consumer tradeoffs

## Decision-Making Frameworks

**1. One-Way vs. Two-Way Door Decisions (Bezos):**

- **One-Way Doors**: Irreversible, require deep analysis, slow deliberation
- **Two-Way Doors**: Reversible, bias toward action, fast iteration
- Default to treating decisions as two-way doors when possible

**2. Disagree and Commit:**

- Voice disagreement clearly
- Once decision is made, commit fully
- Revisit if new data emerges
- Enables faster progress despite imperfect information

**3. DACI Decision Framework:**

- **Driver**: Owns decision process and final call
- **Approver**: Must approve (veto power)
- **Contributors**: Provide input
- **Informed**: Told of decision
- Clarifies who decides vs. who has input

## Resources for Continued Learning

**Essential Books:**

- "Inspired" by Marty Cagan - Product management fundamentals
- "The Lean Startup" by Eric Ries - Build-measure-learn cycle
- "Crossing the Chasm" by Geoffrey Moore - Technology adoption lifecycle
- "The Mom Test" by Rob Fitzpatrick - Customer interview techniques
- "Hooked" by Nir Eyal - Habit-forming product design
- "Escaping the Build Trap" by Melissa Perri - Outcome-driven product management

**Frameworks Origins:**

- RICE Scoring: Intercom
- Jobs-to-be-Done: Clayton Christensen
- North Star Metric: Amplitude/Sean Ellis
- Pirate Metrics (AARRR): Dave McClure
- Opportunity Solution Trees: Teresa Torres

**Industry Benchmarks Sources:**

- SaaS: OpenView Partners, ChartMogul, SaaS Capital
- Consumer: Mixpanel Benchmarks, Amplitude
- NPS: Satmetrix, Bain & Company

## How to Apply This Skill

When helping with product management tasks:

1. **Understand the Context**: Ask clarifying questions about the product stage, industry, users, and goals
2. **Select Appropriate Frameworks**: Choose frameworks that match the problem (early-stage vs. growth-stage requires different approaches)
3. **Adapt to Constraints**: Adjust recommendations based on team size, resources, and timeline
4. **Provide Actionable Guidance**: Move beyond theory to specific next steps
5. **Challenge Assumptions**: Surface unstated assumptions and validate them
6. **Balance Rigor with Speed**: Match analysis depth to decision reversibility
7. **Connect to Outcomes**: Always tie recommendations back to user value and business impact

**Common Product Management Questions:**

- "How should I prioritize these features?" → Apply RICE/ICE scoring with context
- "Is this product-market fit?" → Use PMF assessment framework
- "How do I measure success?" → Define metrics hierarchy and North Star
- "Should I build this feature?" → Customer research + prioritization framework
- "How do I communicate roadmap?" → Stakeholder-specific communication approach
- "What experiment should I run?" → Hypothesis formation + A/B test design
- "How do I know this is the right strategy?" → Vision framework + competitive analysis

This skill embodies world-class Product Management expertise. Apply it rigorously to drive product success from inception through maturity.
